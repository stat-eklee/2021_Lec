{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAk2ihpEfxx_"
   },
   "source": [
    "# 과제4 G202046006 이은경\n",
    "### 해당 자료는 한빛미디어의 \"밑바닥부터 시작하는 딥러닝2\" 서적을 참고하여 작성되었습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_yQRM8rYvfwc",
    "outputId": "a2a3b903-a28b-48f0-e866-ed88e8aacc35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'deep-learning-from-scratch-2' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone  https://github.com/WegraLee/deep-learning-from-scratch-2.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cj7QRM-bxXlD"
   },
   "source": [
    "# 8장 Attention \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8o-_RWbtxdur"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('F:/OneDrive - UOS/2021-1/1. 수업/딥러닝/과제/deep-learning-from-scratch-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 호출\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../ch07')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8 22  9 ...  7  7  7]\n",
      " [17  2  6 ...  7  7  7]\n",
      " [27  1 41 ...  7  7  7]\n",
      " ...\n",
      " [11 11 22 ...  7  7  7]\n",
      " [ 4 37  6 ...  7  7  7]\n",
      " [58  1 49 ... 11  7  7]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s': 0, 'e': 1, 'p': 2, 't': 3, 'm': 4, 'b': 5, 'r': 6, ' ': 7, '2': 8, '7': 9, ',': 10, '1': 11, '9': 12, '4': 13, '_': 14, '-': 15, '0': 16, 'A': 17, 'u': 18, 'g': 19, '3': 20, '8': 21, '/': 22, 'T': 23, 'U': 24, 'E': 25, 'S': 26, 'D': 27, 'Y': 28, 'P': 29, 'M': 30, 'B': 31, 'R': 32, '5': 33, 'J': 34, 'N': 35, '6': 36, 'a': 37, 'i': 38, 'l': 39, 'O': 40, 'c': 41, 'o': 42, 'G': 43, 'F': 44, 'y': 45, 'n': 46, 'C': 47, 'W': 48, 'd': 49, 'I': 50, 'L': 51, 'j': 52, 'H': 53, 'v': 54, 'h': 55, 'V': 56, 'f': 57, 'w': 58}\n"
     ]
    }
   ],
   "source": [
    "print(char_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  7  7 ...  9 22  8]\n",
      " [ 7  7  7 ...  6  2 17]\n",
      " [ 7  7  7 ... 41  1 27]\n",
      " ...\n",
      " [ 7  7  7 ... 22 11 11]\n",
      " [ 7  7  7 ...  6 37  4]\n",
      " [ 7  7 11 ... 49  1 58]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train) # 입력 문장 반전 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "7zBIQsh0IjW0",
    "outputId": "ba4307c1-dfaf-46a5-aa02-473cafe4dd5c"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *  # import numpy as np\n",
    "from common.layers import Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
    "\n",
    "        self.params = self.embed.params + self.lstm.params\n",
    "        self.grads = self.embed.grads + self.lstm.grads\n",
    "        self.hs = None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        self.hs = hs\n",
    "        return hs[:, -1, :]\n",
    "\n",
    "    def backward(self, dh):\n",
    "        dhs = np.zeros_like(self.hs)\n",
    "        dhs[:, -1, :] = dh\n",
    "\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "AiJuHwaZJXOa",
    "outputId": "1b5cf82b-77b2-433c-88a7-262b31c0eef1"
   },
   "outputs": [],
   "source": [
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "7zBIQsh0IjW0",
    "outputId": "ba4307c1-dfaf-46a5-aa02-473cafe4dd5c"
   },
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ar = a.reshape(N, T, 1)#.repeat(T, axis=1)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "\n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "\n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "\n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "7zBIQsh0IjW0",
    "outputId": "ba4307c1-dfaf-46a5-aa02-473cafe4dd5c"
   },
   "outputs": [],
   "source": [
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H)#.repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "\n",
    "        return dhs, dh\n",
    "\n",
    "\n",
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "7zBIQsh0IjW0",
    "outputId": "ba4307c1-dfaf-46a5-aa02-473cafe4dd5c"
   },
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "AiJuHwaZJXOa",
    "outputId": "1b5cf82b-77b2-433c-88a7-262b31c0eef1"
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = None, None\n",
    "\n",
    "    def forward(self, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save_params(self, file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = self.__class__.__name__ + '.pkl'\n",
    "\n",
    "        params = [p.astype(np.float16) for p in self.params]\n",
    "        if GPU:\n",
    "            params = [to_cpu(p) for p in params]\n",
    "\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = self.__class__.__name__ + '.pkl'\n",
    "\n",
    "        if '/' in file_name:\n",
    "            file_name = file_name.replace('/', os.sep)\n",
    "\n",
    "        if not os.path.exists(file_name):\n",
    "            raise IOError('No file: ' + file_name)\n",
    "\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "\n",
    "        params = [p.astype('f') for p in params]\n",
    "        if GPU:\n",
    "            params = [to_gpu(p) for p in params]\n",
    "\n",
    "        for i, param in enumerate(self.params):\n",
    "            param[...] = params[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(BaseModel):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = Decoder(V, D, H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
    "\n",
    "        h = self.encoder.forward(xs)\n",
    "        score = self.decoder.forward(decoder_xs, h)\n",
    "        loss = self.softmax.forward(score, decoder_ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.softmax.backward(dout)\n",
    "        dh = self.decoder.backward(dout)\n",
    "        dout = self.encoder.backward(dh)\n",
    "        return dout\n",
    "\n",
    "    def generate(self, xs, start_id, sample_size):\n",
    "        h = self.encoder.forward(xs)\n",
    "        sampled = self.decoder.generate(h, start_id, sample_size)\n",
    "        return sampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "AiJuHwaZJXOa",
    "outputId": "1b5cf82b-77b2-433c-88a7-262b31c0eef1"
   },
   "outputs": [],
   "source": [
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.layers = None\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T = xs.shape\n",
    "        V, D = self.W.shape\n",
    "\n",
    "        out = np.empty((N, T, D), dtype='f')\n",
    "        self.layers = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Embedding(self.W)\n",
    "            out[:, t, :] = layer.forward(xs[:, t])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, D = dout.shape\n",
    "\n",
    "        grad = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dout[:, t, :])\n",
    "            grad += layer.grads[0]\n",
    "\n",
    "        self.grads[0][...] = grad\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLSTM:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.c = None, None\n",
    "        self.dh = None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        H = Wh.shape[0]\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        if not self.stateful or self.c is None:\n",
    "            self.c = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = LSTM(*self.params)\n",
    "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
    "            hs[:, t, :] = self.h\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D = Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
    "            dxs[:, t, :] = dx\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h, c=None):\n",
    "        self.h, self.c = h, c\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h, self.c = None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        rx = x.reshape(N*T, -1)\n",
    "        out = np.dot(rx, W) + b\n",
    "        self.x = x\n",
    "        return out.reshape(N, T, -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        dout = dout.reshape(N*T, -1)\n",
    "        rx = x.reshape(N*T, -1)\n",
    "\n",
    "        db = np.sum(dout, axis=0)\n",
    "        dW = np.dot(rx.T, dout)\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dx = dx.reshape(*x.shape)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        self.ignore_label = -1\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "\n",
    "        if ts.ndim == 3:  # 정답 레이블이 원핫 벡터인 경우\n",
    "            ts = ts.argmax(axis=2)\n",
    "\n",
    "        mask = (ts != self.ignore_label)\n",
    "\n",
    "        # 배치용과 시계열용을 정리(reshape)\n",
    "        xs = xs.reshape(N * T, V)\n",
    "        ts = ts.reshape(N * T)\n",
    "        mask = mask.reshape(N * T)\n",
    "\n",
    "        ys = softmax(xs)\n",
    "        ls = np.log(ys[np.arange(N * T), ts])\n",
    "        ls *= mask  # ignore_label에 해당하는 데이터는 손실을 0으로 설정\n",
    "        loss = -np.sum(ls)\n",
    "        loss /= mask.sum()\n",
    "\n",
    "        self.cache = (ts, ys, mask, (N, T, V))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ts, ys, mask, (N, T, V) = self.cache\n",
    "\n",
    "        dx = ys\n",
    "        dx[np.arange(N * T), ts] -= 1\n",
    "        dx *= dout\n",
    "        dx /= mask.sum()\n",
    "        dx *= mask[:, np.newaxis]  # ignore_labelㅇㅔ 해당하는 데이터는 기울기를 0으로 설정\n",
    "\n",
    "        dx = dx.reshape((N, T, V))\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t9JeoiwDLTbV",
    "outputId": "95227d76-98ac-46f2-b3d7-9e4edba990ce"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        '''\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Wx: 입력 x에 대한 가중치 매개변수(4개분의 가중치가 담겨 있음)\n",
    "        Wh: 은닉 상태 h에 대한 가장추 매개변수(4개분의 가중치가 담겨 있음)\n",
    "        b: 편향（4개분의 편향이 담겨 있음）\n",
    "        '''\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, H = h_prev.shape\n",
    "\n",
    "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
    "\n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "\n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "\n",
    "        c_next = f * c_prev + g * i\n",
    "        h_next = o * np.tanh(c_next)\n",
    "\n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def backward(self, dh_next, dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "\n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "\n",
    "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "\n",
    "        dc_prev = ds * f\n",
    "\n",
    "        di = ds * g\n",
    "        df = ds * c_prev\n",
    "        do = dh_next * tanh_c_next\n",
    "        dg = ds * i\n",
    "\n",
    "        di *= i * (1 - i)\n",
    "        df *= f * (1 - f)\n",
    "        do *= o * (1 - o)\n",
    "        dg *= (1 - g ** 2)\n",
    "\n",
    "        dA = np.hstack((df, dg, di, do))\n",
    "\n",
    "        dWh = np.dot(h_prev.T, dA)\n",
    "        dWx = np.dot(x.T, dA)\n",
    "        db = dA.sum(axis=0)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        dx = np.dot(dA, Wx.T)\n",
    "        dh_prev = np.dot(dA, Wh.T)\n",
    "\n",
    "        return dx, dh_prev, dc_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2EUP7ZKlvweY",
    "outputId": "c0f83850-8e3a-4dca-99fc-3b809a67f0e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 16[s] | 손실 3.08\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 33[s] | 손실 1.86\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 48[s] | 손실 1.66\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 65[s] | 손실 1.32\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 82[s] | 손실 1.18\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 97[s] | 손실 1.14\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 114[s] | 손실 1.09\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 131[s] | 손실 1.06\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 147[s] | 손실 1.04\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 163[s] | 손실 1.04\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 177[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 193[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 210[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 227[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 243[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 258[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 274[s] | 손실 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1984-01-21\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1984-01-21\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1984-01-21\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1984-01-21\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1984-01-21\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1984-01-21\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1984-01-21\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1984-01-21\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1984-01-21\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1984-01-21\n",
      "---\n",
      "정확도 0.020%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 21 / 351 | 시간 17[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 41 / 351 | 시간 33[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 61 / 351 | 시간 49[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 81 / 351 | 시간 65[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 101 / 351 | 시간 81[s] | 손실 0.97\n",
      "| 에폭 2 |  반복 121 / 351 | 시간 98[s] | 손실 0.96\n",
      "| 에폭 2 |  반복 141 / 351 | 시간 114[s] | 손실 0.95\n",
      "| 에폭 2 |  반복 161 / 351 | 시간 128[s] | 손실 0.92\n",
      "| 에폭 2 |  반복 181 / 351 | 시간 144[s] | 손실 0.89\n",
      "| 에폭 2 |  반복 201 / 351 | 시간 160[s] | 손실 0.87\n",
      "| 에폭 2 |  반복 221 / 351 | 시간 176[s] | 손실 0.83\n",
      "| 에폭 2 |  반복 241 / 351 | 시간 193[s] | 손실 0.80\n",
      "| 에폭 2 |  반복 261 / 351 | 시간 209[s] | 손실 0.75\n",
      "| 에폭 2 |  반복 281 / 351 | 시간 227[s] | 손실 0.69\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 243[s] | 손실 0.62\n",
      "| 에폭 2 |  반복 321 / 351 | 시간 259[s] | 손실 0.53\n",
      "| 에폭 2 |  반복 341 / 351 | 시간 275[s] | 손실 0.43\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2008-11-15\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 2003-05-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1992-12-07\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 2008-08-25\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2007-08-08\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1973-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2016-11-07\n",
      "---\n",
      "정확도 36.940%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 3 |  반복 21 / 351 | 시간 15[s] | 손실 0.29\n",
      "| 에폭 3 |  반복 41 / 351 | 시간 31[s] | 손실 0.21\n",
      "| 에폭 3 |  반복 61 / 351 | 시간 46[s] | 손실 0.15\n",
      "| 에폭 3 |  반복 81 / 351 | 시간 62[s] | 손실 0.11\n",
      "| 에폭 3 |  반복 101 / 351 | 시간 79[s] | 손실 0.09\n",
      "| 에폭 3 |  반복 121 / 351 | 시간 95[s] | 손실 0.07\n",
      "| 에폭 3 |  반복 141 / 351 | 시간 111[s] | 손실 0.05\n",
      "| 에폭 3 |  반복 161 / 351 | 시간 127[s] | 손실 0.04\n",
      "| 에폭 3 |  반복 181 / 351 | 시간 143[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 201 / 351 | 시간 160[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 221 / 351 | 시간 175[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 241 / 351 | 시간 191[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 261 / 351 | 시간 207[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 281 / 351 | 시간 224[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 241[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 321 / 351 | 시간 257[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 341 / 351 | 시간 273[s] | 손실 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.700%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 21 / 351 | 시간 16[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 41 / 351 | 시간 32[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 61 / 351 | 시간 46[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 81 / 351 | 시간 62[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 101 / 351 | 시간 76[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 121 / 351 | 시간 91[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 141 / 351 | 시간 107[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 161 / 351 | 시간 124[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 181 / 351 | 시간 140[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 201 / 351 | 시간 156[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 221 / 351 | 시간 171[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 241 / 351 | 시간 188[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 261 / 351 | 시간 204[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 281 / 351 | 시간 219[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 235[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 321 / 351 | 시간 251[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 341 / 351 | 시간 267[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.880%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 21 / 351 | 시간 18[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 41 / 351 | 시간 34[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 61 / 351 | 시간 49[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 81 / 351 | 시간 66[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 101 / 351 | 시간 82[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 121 / 351 | 시간 97[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 141 / 351 | 시간 113[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 161 / 351 | 시간 128[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 181 / 351 | 시간 144[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 201 / 351 | 시간 161[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 221 / 351 | 시간 177[s] | 손실 0.01\n",
      "| 에폭 5 |  반복 241 / 351 | 시간 194[s] | 손실 0.03\n",
      "| 에폭 5 |  반복 261 / 351 | 시간 211[s] | 손실 0.01\n",
      "| 에폭 5 |  반복 281 / 351 | 시간 227[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 243[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 321 / 351 | 시간 259[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 341 / 351 | 시간 276[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.940%\n",
      "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 21 / 351 | 시간 13[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 41 / 351 | 시간 29[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 61 / 351 | 시간 46[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 81 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 101 / 351 | 시간 79[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 121 / 351 | 시간 94[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 141 / 351 | 시간 112[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 161 / 351 | 시간 128[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 181 / 351 | 시간 144[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 201 / 351 | 시간 161[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 221 / 351 | 시간 176[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 241 / 351 | 시간 192[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 261 / 351 | 시간 207[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 281 / 351 | 시간 221[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 301 / 351 | 시간 237[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 321 / 351 | 시간 254[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 341 / 351 | 시간 270[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.940%\n",
      "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 21 / 351 | 시간 17[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 41 / 351 | 시간 33[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 61 / 351 | 시간 49[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 81 / 351 | 시간 66[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 101 / 351 | 시간 80[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 121 / 351 | 시간 95[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 141 / 351 | 시간 111[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 161 / 351 | 시간 128[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 181 / 351 | 시간 143[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 201 / 351 | 시간 159[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 221 / 351 | 시간 175[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 241 / 351 | 시간 190[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 261 / 351 | 시간 206[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 281 / 351 | 시간 222[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 301 / 351 | 시간 238[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 321 / 351 | 시간 252[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 341 / 351 | 시간 268[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 21 / 351 | 시간 16[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 41 / 351 | 시간 33[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 61 / 351 | 시간 49[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 81 / 351 | 시간 66[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 101 / 351 | 시간 83[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 121 / 351 | 시간 99[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 141 / 351 | 시간 115[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 161 / 351 | 시간 132[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 181 / 351 | 시간 148[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 201 / 351 | 시간 163[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 221 / 351 | 시간 179[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 241 / 351 | 시간 195[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 261 / 351 | 시간 211[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 281 / 351 | 시간 226[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 301 / 351 | 시간 242[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 321 / 351 | 시간 259[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 341 / 351 | 시간 275[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 21 / 351 | 시간 17[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 41 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 61 / 351 | 시간 48[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 81 / 351 | 시간 64[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 101 / 351 | 시간 79[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 121 / 351 | 시간 96[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 141 / 351 | 시간 112[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 161 / 351 | 시간 128[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 181 / 351 | 시간 144[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 201 / 351 | 시간 159[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 221 / 351 | 시간 175[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 241 / 351 | 시간 190[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 261 / 351 | 시간 207[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 281 / 351 | 시간 223[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 301 / 351 | 시간 238[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 321 / 351 | 시간 255[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 341 / 351 | 시간 270[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 21 / 351 | 시간 16[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 41 / 351 | 시간 32[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 61 / 351 | 시간 48[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 81 / 351 | 시간 63[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 101 / 351 | 시간 79[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 121 / 351 | 시간 96[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 141 / 351 | 시간 113[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 161 / 351 | 시간 128[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 181 / 351 | 시간 142[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 201 / 351 | 시간 156[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 221 / 351 | 시간 171[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 241 / 351 | 시간 188[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 261 / 351 | 시간 202[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 281 / 351 | 시간 218[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 301 / 351 | 시간 235[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 321 / 351 | 시간 251[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 341 / 351 | 시간 267[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-bed5a4676beb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-9e8a72d198e5>\u001b[0m in \u001b[0;36msave_params\u001b[1;34m(self, file_name)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('정확도 %.3f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "FJuvPkeALWN9",
    "outputId": "91681e12-1c1d-4af4-86f9-cd82a40e638f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 50640 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 54253 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 51221 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 54869 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 46020 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 50640 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 54253 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 51221 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 54869 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 46020 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('에폭')\n",
    "plt.ylabel('정확도')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "과제3_G202046006_이은경_ipynb의_사본의_사본.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
